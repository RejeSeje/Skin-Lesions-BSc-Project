{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, ConfusionMatrixDisplay, confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "from CNN import test_resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_file_pairs(base_path=\"data/splitted_csv\",\n",
    "                        test_sets=5, variations=['0.00', '0.25', '0.50', '0.75', '1.00'], train_val_pairs=5):\n",
    "    # Dictionary to hold all train-validation-test triples for all test sets\n",
    "    all_triples = {}\n",
    "    base_path = \"/Users/niko/Documents/uni/6. semester/bsc/Project/BSc-Project/data/splitted_csv\"\n",
    "\n",
    "    # Generate file names\n",
    "    for test_set in range(test_sets):\n",
    "        triples = []\n",
    "\n",
    "        # Generating test set filename\n",
    "        test_filename = f'{base_path}/m_f_ca_nc_test_{test_set}.csv'\n",
    "\n",
    "        for variation in variations:\n",
    "            for pair in range(train_val_pairs):\n",
    "                # Include the base path in the filename\n",
    "                train_filename = f'{base_path}/m_f_ca_nc_train_{test_set}_{variation}_{pair}.csv'\n",
    "                val_filename = f'{base_path}/m_f_ca_nc_val_{test_set}_{variation}_{pair}.csv'\n",
    "                triples.append((train_filename, val_filename, test_filename))\n",
    "        \n",
    "        all_triples[f'test_set_{test_set}'] = triples\n",
    "    \n",
    "    return all_triples\n",
    "\n",
    "# Example usage\n",
    "all_file_triples = generate_file_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5002\")  # Ensure no extra spaces or slashes beyond this\n",
    "\n",
    "\n",
    "mlflow.set_experiment(\"CNN_Tester\")\n",
    "\n",
    "def load_dataset(filepath):\n",
    "    # Adjust this function to fit how your data is structured\n",
    "    # For example, load the CSV file and return features and labels\n",
    "    cancer = ['SCC', 'BCC', 'MEL']\n",
    "\n",
    "\n",
    "    def is_cancerous(condition):\n",
    "        return any(cancer in condition for cancer in cancer)\n",
    "    \n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    df['is_cancerous'] = df['diagnostic'].apply(is_cancerous).astype(int)\n",
    "\n",
    "    # \"pigment_network_coverage\", \"blue_veil_pixels\", \"globules_count\", \"streaks_irregularity\",\n",
    "    # \"irregular_pigmentation_coverage\", \"regression_pixels\", \"compactness_x\", \"avg_red_channel\", \n",
    "    # \"avg_green_channel\", \"avg_blue_channel\", \"multicolor_rate\", \"asymmetry\", \"average_hue\",\n",
    "    # \"average_saturation\", \"average_value\", \"mean_asymmetry\", \"best_asymmetry\", \"worst_asymmetry\",\n",
    "    # \"red_var\", \"green_var\", \"blue_var\", \"hue_var\", \"sat_var\", \"val_var\", \"dom_hue\", \"dom_sat\", \n",
    "    # \"dom_val\", \"compactness_y\", \"convexity\", \"F1\", \"F2\", \"F3\", \"F10\", \"F11\", \"F12\"\n",
    "    \n",
    "    X = df[[\"img_id\"]]  # Features\n",
    "    y = df[[\"is_cancerous\"]]   # Labels\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(file_triples):\n",
    "    results = []\n",
    "    count = 0\n",
    "    img_path = \"/Users/niko/Documents/uni/6. semester/bsc/Project/BSc-Project/pad-ufes/images\"\n",
    "    for test_set, triples in file_triples.items():\n",
    "        for train_file, val_file, test_file in triples:\n",
    "            X_val, y_val = load_dataset(val_file)\n",
    "            X_test, y_test = load_dataset(test_file)\n",
    "            count += 1\n",
    "\n",
    "            with mlflow.start_run(run_name=f\"cnn_{count}\"):\n",
    "                # Define model and pipeline elements here as needed\n",
    "                print('before cnn')\n",
    "\n",
    "                y_pred, cnn = test_resnet50(train_file, val_file, test_file, img_path)\n",
    "\n",
    "                print('after cnn')\n",
    "                #y_pred = cnn.predict(X_test)\n",
    "\n",
    "                #mlflow.log_params(cnn.best_params_)\n",
    "                #mlflow.log_metric(\"accuracy\", search.best_score_)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                #print(\"Accuracy for all groups:\", accuracy)\n",
    "                #print(\"Best gridsearch score\", search.best_score_)\n",
    "                #print(f\"Penalty {search.best_params_[\"logistic__penalty\"]}, \\\n",
    "                #Solver {search.best_params_[\"logistic__solver\"]}, \\\n",
    "                #Fit intercept {search.best_params_[\"logistic__fit_intercept\"]}, \\\n",
    "                #C {search.best_params_[\"logistic__C\"]}, \\\n",
    "                #Class weight {search.best_params_[\"logistic__class_weight\"]}, \\\n",
    "                #Max iterations {search.best_params_[\"logistic__max_iter\"]}\")\n",
    "                \n",
    "\n",
    "                # cm = confusion_matrix(y_val, y_pred)\n",
    "                # cm = ConfusionMatrixDisplay(cm)\n",
    "                # cm = cm.plot()\n",
    "                # plt.savefig(f\"confusion_matrix_{count}.png\")\n",
    "                # plt.show()\n",
    "\n",
    "                # feature_importance = np.abs(search.best_estimator_.named_steps['logistic'].coef_[0])\n",
    "                # feature_names = X_train.columns\n",
    "                # plt.figure(figsize=(10, 8))\n",
    "                # plt.barh(feature_names, feature_importance)\n",
    "                # plt.xlabel('Coefficient Magnitude')\n",
    "                # plt.ylabel('Features')\n",
    "                # plt.title('Feature Importance for Logistic Regression Model')\n",
    "                # plt.tight_layout()\n",
    "                # mlflow.log_figure(plt.gcf(), \"feature_importance\")\n",
    "\n",
    "                \n",
    "                metrics = [\n",
    "                (\"ROC\", roc_auc_score(y_test, y_pred), []),\n",
    "                (\"Accuracy\", accuracy_score(y_test, y_pred), []),\n",
    "                (\"Recall\", recall_score(y_test, y_pred), []),\n",
    "                (\"Precision\", precision_score(y_test, y_pred), []),\n",
    "                (\"F1-score\", f1_score(y_test, y_pred), [])\n",
    "                ]\n",
    "\n",
    "                for name, value, _ in metrics:\n",
    "                    mlflow.log_metric(name, value)\n",
    "\n",
    "\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                variation = train_file.split('_')[-2]  # This will extract the variation component based on your path format\n",
    "                results.append({\n",
    "                    \"variation\": variation,\n",
    "                    # \"train_file\": train_file,\n",
    "                    # \"validation_file\": val_file,\n",
    "                    # \"test_file\": test_file,\n",
    "                    \"accuracy\": accuracy,\n",
    "                    #\"best_params\": search.best_params_,\n",
    "                    #\"best_score\": search.best_score_,\n",
    "                    \"ROC/AUC score\": roc_auc_score(y_test, y_pred), \n",
    "                })\n",
    "                \n",
    "\n",
    "\n",
    "                # mlflow.log_figure(cm, \"confusion_matrix\")\n",
    "\n",
    "                # mlflow.log_artifact(f\"confusion_matrix_{count}.png\", \"confusion_matrices\")\n",
    "\n",
    "                \n",
    "                # Log model and metrics\n",
    "                mlflow.sklearn.log_model(cnn, \"cnn\")\n",
    "\n",
    "    # Convert results to DataFrame and save to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('model_training_results_cnn.csv', index=False)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/Users/niko/Documents/uni/6. semester/bsc/Project/BSc-Project/data/splitted_csv\"\n",
    "all_file_triples = generate_file_pairs(base_path=base_path)#base_path=base_path\n",
    "results = train_and_evaluate_model(all_file_triples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelorenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
