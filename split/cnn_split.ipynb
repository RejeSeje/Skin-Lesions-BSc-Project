{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspiration taken from https://github.com/e-pet/adni-bias/blob/main/Repeated_CV_Splitter.py\n",
    "\n",
    "Similar to lr_split_aug.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import warn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from image_augmentation import image_augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training, validation and test tests for CNN, aka just keeping img_id, gender, is_cancerous and fitzpatrick with no synthetic data, but additional data from augmented images. Can split twice:\n",
    "\n",
    "1: Augmenting each image at most one time\n",
    "\n",
    "2: Augmenting each image the minimum amount of times necessairy to achieve an even 50/50 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ca_nc_split_dfs(df, split_col):\n",
    "    col_vals = df[split_col].unique() # Gets the unique values for the coloumn we choose to split our data on\n",
    "    col_vals.sort()\n",
    "    assert (len(col_vals) == 2) # Checks that our value (sex) is binary in our dataframe \n",
    "    a_df = df[df[split_col] == col_vals[0]] # Female, since it's sorted alphabetically\n",
    "    b_df = df[df[split_col] == col_vals[1]] # Male\n",
    "    a_nc_df = a_df[a_df['diagnostic'].isin(non_cancerous_conditions)]\n",
    "    a_ca_df = a_df[a_df['diagnostic'].isin(cancerous_conditions)]\n",
    "    b_nc_df = b_df[b_df['diagnostic'].isin(non_cancerous_conditions)]\n",
    "    b_ca_df = b_df[b_df['diagnostic'].isin(cancerous_conditions)]\n",
    "    return a_ca_df, b_ca_df, a_nc_df, b_nc_df\n",
    "\n",
    "\n",
    "def get_train_set_sizes(ca_a_df, ca_b_df, nc_a_df, nc_b_df, ratio, train_set_size, cancerous_fraction=None):\n",
    "    if cancerous_fraction is None:\n",
    "        ca_a_fraction = len(ca_a_df) / (len(ca_a_df) + len(nc_a_df))\n",
    "        ca_b_fraction = len(ca_b_df) / (len(ca_b_df) + len(nc_b_df))\n",
    "        warn(\n",
    "            \"Using legacy group-wise disease label stratification. Will lead to different disease prevalences in the different groups in the training set. NOT RECOMMENDED.\")\n",
    "    else:\n",
    "        ca_a_fraction = ca_b_fraction = cancerous_fraction\n",
    "\n",
    "    nc_a_fraction = 1 - ca_a_fraction\n",
    "    nc_b_fraction = 1 - ca_b_fraction\n",
    "    train_set_n_ca_a_nom = ca_a_fraction * train_set_size * ratio\n",
    "    train_set_n_ca_a = round(train_set_n_ca_a_nom)\n",
    "    train_set_n_ca_b_nom = ca_b_fraction * train_set_size * (1 - ratio)\n",
    "    train_set_n_ca_b = round(train_set_n_ca_b_nom)\n",
    "    train_set_n_nc_a_nom = nc_a_fraction * train_set_size * ratio\n",
    "    train_set_n_nc_a = round(train_set_n_nc_a_nom)\n",
    "    train_set_n_nc_b_nom = nc_b_fraction * train_set_size * (1 - ratio)\n",
    "    train_set_n_nc_b = round(train_set_n_nc_b_nom)\n",
    "\n",
    "    while train_set_n_ca_a + train_set_n_ca_b + train_set_n_nc_a + train_set_n_nc_b < train_set_size:\n",
    "        diffs = [train_set_n_ca_a_nom - train_set_n_ca_a, train_set_n_ca_b_nom - train_set_n_ca_b,\n",
    "                 train_set_n_nc_a_nom - train_set_n_nc_a, train_set_n_nc_b_nom - train_set_n_nc_b]\n",
    "        max_diff_idx = diffs.index(max(diffs))\n",
    "        if max_diff_idx == 0:\n",
    "            train_set_n_ca_a += 1\n",
    "        elif max_diff_idx == 1:\n",
    "            train_set_n_ca_b += 1\n",
    "        elif max_diff_idx == 2:\n",
    "            train_set_n_nc_a += 1\n",
    "        else:\n",
    "            train_set_n_nc_b += 1\n",
    "\n",
    "    while train_set_n_ca_a + train_set_n_ca_b + train_set_n_nc_a + train_set_n_nc_b > train_set_size:\n",
    "        diffs = [train_set_n_ca_a_nom - train_set_n_ca_a, train_set_n_ca_b_nom - train_set_n_ca_b,\n",
    "                 train_set_n_nc_a_nom - train_set_n_nc_a, train_set_n_nc_b_nom - train_set_n_nc_b]\n",
    "        min_diff_idx = diffs.index(min(diffs))\n",
    "        if min_diff_idx == 0:\n",
    "            train_set_n_ca_a -= 1\n",
    "        elif min_diff_idx == 1:\n",
    "            train_set_n_ca_b -= 1\n",
    "        elif min_diff_idx == 2:\n",
    "            train_set_n_nc_a -= 1\n",
    "        else:\n",
    "            train_set_n_nc_b -= 1\n",
    "\n",
    "    assert (train_set_n_ca_a + train_set_n_ca_b + train_set_n_nc_a + train_set_n_nc_b == train_set_size)\n",
    "\n",
    "    return train_set_n_ca_a, train_set_n_ca_b, train_set_n_nc_a, train_set_n_nc_b\n",
    "\n",
    "\n",
    "def assign_test_sets(df, n_test_sets, condition, rng):\n",
    "\n",
    "    n_reruns = int(np.ceil(test_size_per_sex_per_group * n_test_sets / len(df)))\n",
    "    for test_idx in range(0, n_test_sets):\n",
    "        test_set_name = 'test_set_' + str(test_idx)\n",
    "        df[test_set_name] = 0\n",
    "\n",
    "    for rerun_idx in range(0, n_reruns):\n",
    "        rerun_name = 'rerun_' + str(rerun_idx)\n",
    "        df[rerun_name] = 0\n",
    "    \n",
    "    df_without_aug = df[~df['img_id'].str.startswith('aug_')] # Makes a dataframe without the augmented data, so test sets are made with wholly original data\n",
    "    df_with_aug = df[df['img_id'].str.startswith('aug_')] # Makes a dataframe with only the augmented data, to be concatenated when test splitting is done\n",
    "\n",
    "    \n",
    "    for rerun_idx in range(0, n_reruns):\n",
    "        rerun_name = 'rerun_' + str(rerun_idx)\n",
    "\n",
    "        grouped_patient = df_without_aug.groupby('patient_id') # Group lesions from the same patient to ensure that they are not distributed across different tests or train/validation df\n",
    "\n",
    "        for test_idx in range(0, n_test_sets):\n",
    "            test_set_name = 'test_set_' + str(test_idx)\n",
    "\n",
    "            while complement_lesions_to_use[test_idx]:\n",
    "                patient_id = complement_lesions_to_use[test_idx].pop(0)\n",
    "                lesion = grouped_patient.get_group(patient_id)\n",
    "                remaining = sum(df_without_aug[rerun_name] == 0)\n",
    "                missing = test_size_per_sex_per_group - sum(df_without_aug[test_set_name])\n",
    "                eligibles = lesion[(lesion[test_set_name] == 0) & (lesion[rerun_name] == 0)]\n",
    "                eligible_count = len(eligibles)\n",
    "                sample_size = min([missing, min([remaining, int(np.ceil(len(df_without_aug) / n_reruns))])])\n",
    "                sample_size = min(sample_size, eligible_count)  # Ensure sample size doesn't exceed eligible items\n",
    "                local_sample = eligibles.sample(n=sample_size, replace=False, random_state=rng)\n",
    "\n",
    "                df_without_aug.loc[local_sample.index, test_set_name] = 1\n",
    "                df_without_aug.loc[local_sample.index, rerun_name] = 1\n",
    "            \n",
    "            for patient_id, lesion in grouped_patient:\n",
    "                remaining = sum(df_without_aug[rerun_name] == 0)\n",
    "                missing = test_size_per_sex_per_group - sum(df_without_aug[test_set_name])\n",
    "                if missing < test_size_per_sex_per_group - 7 and 1 in lesion: # Only add patients with both cancerous and non-cancerous lesions in the first half of sampling process, to make sure there aren't too many complements to be used in the corresponding test set\n",
    "                    continue\n",
    "                if condition == 1 and 1 in lesion['both_cancerous_and_non_cancerous'].values: # Skip to next iteration if patient has both cancerous and non-cancerous lesions and we are in cancer set, as no more data is added afterwards, and therefore can't add more pairs\n",
    "                    continue\n",
    "                if (len(lesion) > missing): # Skip to next iteration if more lesions are grouped than are missing\n",
    "                    continue                   \n",
    "                if 1 in lesion['both_cancerous_and_non_cancerous'].values:\n",
    "                    complement_lesions_to_use[test_idx].append(patient_id)\n",
    "                eligibles = lesion[(lesion[test_set_name] == 0) & (lesion[rerun_name] == 0)]    \n",
    "\n",
    "                eligible_count = len(eligibles)\n",
    "                sample_size = min([missing, min([remaining, int(np.ceil(len(df_without_aug) / n_reruns))])])\n",
    "                sample_size = min(sample_size, eligible_count)  # Ensure sample size doesn't exceed eligible items\n",
    "                local_sample = eligibles.sample(n=sample_size, replace=False, random_state=rng)\n",
    "\n",
    "                df_without_aug.loc[local_sample.index, test_set_name] = 1\n",
    "                df_without_aug.loc[local_sample.index, rerun_name] = 1     \n",
    "        if condition == 1:\n",
    "            assert (complement_lesions_to_use[test_idx] == []) # Ensure that all lesions from same patient are used in the same test set\n",
    "\n",
    "    df = pd.concat([df_without_aug, df_with_aug]) # Concatenate the augmented data back to the dataframe\n",
    "\n",
    "    for test_idx in range(0, n_test_sets):\n",
    "        test_set_name = 'test_set_' + str(test_idx)\n",
    "        assert (sum(df[test_set_name] == 1) == test_size_per_sex_per_group)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def check_unique(df) -> None:\n",
    "    if 'lesion_id' in df.columns:\n",
    "        assert (df.lesion_id.is_unique)\n",
    "    if 'img_id' in df.columns:\n",
    "        assert (df.img_id.is_unique)        \n",
    "    if 'lesion_id' not in df.columns and 'img_id' not in df.columns:\n",
    "        warn(\"could not check subject uniqueness since neither 'lesion_id' nor 'img_id' column present\")    \n",
    "\n",
    "\n",
    "def sort_df(df) -> None:\n",
    "    if 'lesion_id' in df.columns:\n",
    "        return df.sort_values('lesion_id')\n",
    "    if 'img_id' in df.columns:\n",
    "        return df.sort_values('img_id')\n",
    "    if df.index.name in ['lesion_id', 'img_id']:\n",
    "        return df.sort_index()\n",
    "    if 'lesion_id' not in df.columns and 'img_id' not in df.columns and df.index.name not in ['lesion_id', 'img_id']:\n",
    "        warn(\"could not check subject uniqueness since neither 'lesion_id' nor 'img_id' column present\")\n",
    "\n",
    "def clean_data(df):\n",
    "    df = df.sort_values([\"lesion_id\"]) \n",
    "    df = df[~df[\"gender\"].isna()] # Removing data without gender entry\n",
    "    df = df.drop_duplicates(\"lesion_id\", keep = \"first\") # Dropping duplicates, but keeping the first occurence of the lesion_id\n",
    "\n",
    "    df['is_cancerous'] = df['diagnostic'].apply(lambda x: any(cancer in x for cancer in ['SCC', 'BCC', 'MEL'])).astype(int) # Add is_cancerous column to df, with 1 being cancerous and 0 being non-cancerous\n",
    "\n",
    "    # Remove unnecessary columns for cleanliness\n",
    "    df = df[['patient_id', 'img_id', 'lesion_id', 'gender', 'fitspatrick', 'is_cancerous', 'diagnostic']] # Pad-Ufes has misspelled fitzpatrick and I can't be bothered to change all our datasets\n",
    "    return df\n",
    "\n",
    "def get_augmented_data(df):\n",
    "    # Call function for balancing dataset via augmented data\n",
    "    df_once_augmented, df_50_50 = image_augmentation.balance_dataset(df, 'is_cancerous')\n",
    "    \n",
    "    return df_once_augmented, df_50_50\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    n_test_sets = 5\n",
    "    ratios = [0, 0.25, 0.5, 0.75, 1.0]\n",
    "    n_folds = 5\n",
    "    cancerous_conditions = [\"BCC\", \"MEL\", \"SCC\"]\n",
    "    non_cancerous_conditions = [\"ACK\", \"NEV\", \"SEK\"]\n",
    "    cancer = 1\n",
    "    non_cancer = 0\n",
    "    genders = [\"FEMALE\", \"MALE\"]\n",
    "    rng = np.random.default_rng(1173).bit_generator\n",
    "    complement_lesions_to_use = {}\n",
    "    basepaths = ['../data/cnn/cnn_splitted_data_once_augmented/',  '../data/cnn/cnn_splitted_data_50_50_split/'] # List of basepaths for the two different augmented datasets\n",
    "\n",
    "    test_size_per_sex_per_group = 26 # 26 each of male/female non-cancerous/cancerous patients, i.e., total test size is 104\n",
    "\n",
    "    for i in range(0, n_test_sets): # Create a list for each test set to store the complement lesions to use\n",
    "        complement_lesions_to_use[i] = []\n",
    "\n",
    "    df = pd.read_csv(\"../data/metadata/fixed_metadata.csv\") # Dataframe with all metadata for the PAD-UFES-20 dataset where duplicate lesion_id's have been renamed\n",
    "    df = clean_data(df) # Clean data after feature extraction\n",
    "    df_once_augmented, df_50_50 = get_augmented_data(df) # add augmented images to dataset (df 50/50 not used in this cell)\n",
    "\n",
    "    dfs = [df_once_augmented, df_50_50] # List of dataframes to iterate over\n",
    "\n",
    "    df_once_augmented.to_csv('../data/metadata/once_augmented.csv', index=False)\n",
    "    df_50_50.to_csv('../data/metadata/50_50_augmented.csv', index=False)\n",
    "\n",
    "    # for i in range(len(basepaths)):\n",
    "    basepath = basepaths[0]\n",
    "    df = dfs[0]\n",
    "\n",
    "    basename = basepath + 'm_f_ca_nc'\n",
    "    check_unique(df)\n",
    "\n",
    "    # Biggest training set possible, found by trial and error\n",
    "    train_set_size = 535\n",
    "\n",
    "    # Create boolean masks for cancerous and non-cancerous conditions\n",
    "    cancerous_mask = df['diagnostic'].isin(cancerous_conditions)\n",
    "    non_cancerous_mask = df['diagnostic'].isin(non_cancerous_conditions)\n",
    "\n",
    "    # Calculate the fraction of cancerous conditions among all conditions\n",
    "    cancerous_fraction = sum(cancerous_mask) / (sum(cancerous_mask) + sum(non_cancerous_mask))\n",
    "\n",
    "    # Make dataframe with only non-cancerous conditions\n",
    "    df_nc = df[non_cancerous_mask]\n",
    "\n",
    "    # Check for whether a patient has both cancerous and non cancerous lesions \n",
    "    nc_ca_df = df.groupby(['patient_id'])['is_cancerous'].nunique()\n",
    "\n",
    "    # Filter groups where both True (cancerous) and False (non-cancerous) exist\n",
    "    nc_ca_df = nc_ca_df[nc_ca_df == 2]\n",
    "\n",
    "    # List of patient_id's with both cancerous and non-cancerous lesions\n",
    "    nc_ca_df_patient_ids = nc_ca_df.index.tolist()\n",
    "\n",
    "    # Add identifier to the dataframe for patients with both cancerous and non-cancerous lesions\n",
    "    df['both_cancerous_and_non_cancerous'] = 0\n",
    "    df.loc[df['patient_id'].isin(nc_ca_df_patient_ids), 'both_cancerous_and_non_cancerous'] = 1\n",
    "\n",
    "    # Split data into dataframes on gender and diagnostic\n",
    "    ca_a_df, ca_b_df, nc_a_df, nc_b_df = get_ca_nc_split_dfs(df, 'gender') #ca: cancerous, nc: non-cancerous, a: female, b: male\n",
    "\n",
    "    # Compute cross-validation fold sizes\n",
    "    fold_size_base, fold_size_rem = divmod(train_set_size, n_folds)\n",
    "    fold_sizes = []\n",
    "    for ii in range(0, n_folds):\n",
    "        if ii < fold_size_rem:\n",
    "            fold_sizes.append(fold_size_base + 1)\n",
    "        else:\n",
    "            fold_sizes.append(fold_size_base)\n",
    "    assert (sum(fold_sizes) == train_set_size)\n",
    "\n",
    "    # Set up the desired test sets\n",
    "    nc_a_df = assign_test_sets(nc_a_df, n_test_sets, non_cancer, rng)\n",
    "    ca_a_df = assign_test_sets(ca_a_df, n_test_sets, cancer, rng)\n",
    "    nc_b_df = assign_test_sets(nc_b_df, n_test_sets, non_cancer, rng)\n",
    "    ca_b_df = assign_test_sets(ca_b_df, n_test_sets, cancer, rng)\n",
    "\n",
    "    for test_idx in range(0, n_test_sets):\n",
    "\n",
    "        test_set_name = 'test_set_' + str(test_idx)\n",
    "\n",
    "        test_df = sort_df(pd.concat([ca_a_df[ca_a_df[test_set_name] == 1],\n",
    "                                    nc_a_df[nc_a_df[test_set_name] == 1],\n",
    "                                    ca_b_df[ca_b_df[test_set_name] == 1],\n",
    "                                    nc_b_df[nc_b_df[test_set_name] == 1]]))\n",
    "\n",
    "        check_unique(test_df)\n",
    "\n",
    "        test_df.to_csv(basename + f'_test_{test_idx}.csv')\n",
    "\n",
    "        # Mark corresponding augmented lesions of the ones used in test as used in test set as well\n",
    "        img_with_corresponding_aug = test_df['img_id'].tolist() \n",
    "        img_with_corresponding_aug = ['aug_' + img_id for img_id in img_with_corresponding_aug]\n",
    "        nc_a_df.to_csv(f'../test/test_csvs/before_addition_{test_idx}.csv')\n",
    "\n",
    "        for img_id in img_with_corresponding_aug: # Since we only augment non-cancerous lesions, we only need to check these\n",
    "            if img_id in nc_a_df['img_id'].values: \n",
    "                nc_a_df.loc[nc_a_df['img_id'] == img_id, test_set_name] = 1\n",
    "            if img_id in nc_b_df['img_id'].values:\n",
    "                nc_b_df.loc[nc_b_df['img_id'] == img_id, test_set_name] = 1\n",
    "\n",
    "        # Mark that nothing has been used in the training / validation sets belong to this test set yet\n",
    "        ca_a_df.loc[:, 'used_with_curr_test'] = 0\n",
    "        ca_b_df.loc[:, 'used_with_curr_test'] = 0\n",
    "        nc_a_df.loc[:, 'used_with_curr_test'] = 0\n",
    "        nc_b_df.loc[:, 'used_with_curr_test'] = 0\n",
    "\n",
    "        ratios.sort()\n",
    "        # Set up the training and validation sets for each test set\n",
    "        for ratio in ratios:\n",
    "            # ----- Determine (based on ratio and train_set_size) how many CA/NC F/M there should be and\n",
    "            # split into the four corresponding DFs.\n",
    "            train_set_n_ca_a, train_set_n_ca_b, train_set_n_nc_a, train_set_n_nc_b = \\\n",
    "                get_train_set_sizes(ca_a_df, ca_b_df, nc_a_df, nc_b_df, ratio, train_set_size,\n",
    "                                    cancerous_fraction=cancerous_fraction)\n",
    "\n",
    "            # Compose a training + validation dataset with the desired sex ratio from the remaining non-test data\n",
    "            # Reuse samples that have been used for earlier ratios wherever possible to minimize training set\n",
    "            # variations across ratios.\n",
    "            if ratio == min(ratios):\n",
    "                # first ratio, just sample from scratch\n",
    "                train_ca_a_df = ca_a_df[ca_a_df[test_set_name] == 0].sample(n=train_set_n_ca_a, random_state=rng)\n",
    "                train_ca_b_df = ca_b_df[ca_b_df[test_set_name] == 0].sample(n=train_set_n_ca_b, random_state=rng)\n",
    "                train_nc_a_df = nc_a_df[nc_a_df[test_set_name] == 0].sample(n=train_set_n_nc_a, random_state=rng)\n",
    "                train_nc_b_df = nc_b_df[nc_b_df[test_set_name] == 0].sample(n=train_set_n_nc_b, random_state=rng)\n",
    "            else:\n",
    "                # We work with increasing ratios, i.e., we now have less males and more females than for the\n",
    "                # previous ratio.\n",
    "                # Draw males only from the ones that have been used so far\n",
    "                train_ca_b_df = ca_b_df[ca_b_df['used_with_curr_test'] == 1].sample(n=train_set_n_ca_b,\n",
    "                                                                                    random_state=rng)\n",
    "                train_nc_b_df = nc_b_df[nc_b_df['used_with_curr_test'] == 1].sample(n=train_set_n_nc_b,\n",
    "                                                                                    random_state=rng)\n",
    "                # Use all females used so far + draw new ones as needed\n",
    "                n_prev = ca_a_df['used_with_curr_test'].sum()\n",
    "                train_ca_a_df = pd.concat([ca_a_df[ca_a_df['used_with_curr_test'] == 1],\n",
    "                                            ca_a_df[(ca_a_df[test_set_name] == 0) & (\n",
    "                                                        ca_a_df['used_with_curr_test'] == 0)].sample(\n",
    "                                                n=train_set_n_ca_a - n_prev, random_state=rng)])\n",
    "                n_prev = nc_a_df['used_with_curr_test'].sum()\n",
    "                train_nc_a_df = pd.concat([nc_a_df[nc_a_df['used_with_curr_test'] == 1],\n",
    "                                            nc_a_df[(nc_a_df[test_set_name] == 0) & (\n",
    "                                                        nc_a_df['used_with_curr_test'] == 0)].sample(\n",
    "                                                n=train_set_n_nc_a - n_prev, random_state=rng)])\n",
    "\n",
    "            # Mark which ones we have used so far with the current test set + this and previous ratios\n",
    "            ca_a_df.loc[train_ca_a_df.index, 'used_with_curr_test'] = 1\n",
    "            ca_b_df.loc[train_ca_b_df.index, 'used_with_curr_test'] = 1\n",
    "            nc_a_df.loc[train_nc_a_df.index, 'used_with_curr_test'] = 1\n",
    "            nc_b_df.loc[train_nc_b_df.index, 'used_with_curr_test'] = 1\n",
    "\n",
    "            train_and_vali_df = sort_df(pd.concat([train_ca_a_df, train_ca_b_df, train_nc_a_df, train_nc_b_df]))\n",
    "            \n",
    "\n",
    "            # Set up folds for cross-validation\n",
    "            train_and_vali_df['fold'] = np.nan\n",
    "            # Randomly sampling augmented data with a corresponding value so each fold has the same amount of augmented data\n",
    "            aug_train_and_vali_df = train_and_vali_df[train_and_vali_df['img_id'].str.startswith('aug_')]\n",
    "            img_ids_train_and_vali = train_and_vali_df[~train_and_vali_df['img_id'].str.startswith('aug_')]['img_id'].tolist()\n",
    "\n",
    "            # remove prefix from all img_ids in aug for easier comparison\n",
    "            aug_train_and_vali_df['img_id'] = aug_train_and_vali_df['img_id'].str.replace('aug_', '')\n",
    "\n",
    "            # Only keep values in aug where img_id is in img_ids\n",
    "            aug_train_and_vali_df = aug_train_and_vali_df[aug_train_and_vali_df['img_id'].isin(img_ids_train_and_vali)]\n",
    "\n",
    "            # Add prefix back to img_ids in aug\n",
    "            aug_train_and_vali_df['img_id'] = 'aug_' + aug_train_and_vali_df['img_id']\n",
    "\n",
    "            len_aug = len(aug_train_and_vali_df)\n",
    "\n",
    "            fold_aug_size_base, fold_aug_size_rem = divmod(len_aug, n_folds)\n",
    "            fold_aug_sizes = []\n",
    "            for iii in range(0, n_folds):\n",
    "                if iii < fold_aug_size_rem:\n",
    "                    fold_aug_sizes.append(fold_aug_size_base + 1)\n",
    "                else:\n",
    "                    fold_aug_sizes.append(fold_aug_size_base)\n",
    "            assert (sum(fold_aug_sizes) == len_aug)\n",
    "\n",
    "            for fold_aug_idx, fold_aug_size in enumerate(fold_aug_sizes):\n",
    "                # Split augmented data into folds\n",
    "                aug_train_and_vali_df.loc[aug_train_and_vali_df[aug_train_and_vali_df.fold.isna()].sample(n=fold_aug_size,\n",
    "                                                                                                random_state=rng).index, 'fold'] = fold_aug_idx\n",
    "            \n",
    "            \n",
    "            train_and_vali_df = train_and_vali_df.set_index('img_id')\n",
    "            aug_train_and_vali_df = aug_train_and_vali_df.set_index('img_id')\n",
    "            train_and_vali_df.update(aug_train_and_vali_df)\n",
    "            aug_train_and_vali_df = aug_train_and_vali_df.reset_index()\n",
    "            aug_train_and_vali_df['img_id'] = aug_train_and_vali_df['img_id'].str.replace('aug_', '')\n",
    "            \n",
    "            # Assign the fold of the augmented image to the corresponding image\n",
    "            fold_map = aug_train_and_vali_df.set_index('img_id')['fold'].to_dict()\n",
    "\n",
    "            train_and_vali_df['fold'] = train_and_vali_df.index.to_series().apply(lambda x: fold_map.get(x, train_and_vali_df.loc[x, 'fold']))\n",
    "            train_and_vali_df = train_and_vali_df.reset_index()\n",
    "\n",
    "            for fold_idx, fold_size in enumerate(fold_sizes):\n",
    "                assert (train_and_vali_df.index.is_unique)\n",
    "                # Amount of data added to fold so far from augs is then subtracted from fold_size\n",
    "                train_and_vali_df.loc[train_and_vali_df[train_and_vali_df.fold.isna()].sample(n=fold_size - fold_aug_sizes[fold_idx] * 2,\n",
    "                                                                                                random_state=rng).index, 'fold'] = fold_idx\n",
    "            \n",
    "            # check that everything looks nice\n",
    "            check_unique(test_df)\n",
    "            check_unique(train_and_vali_df)\n",
    "            assert (~train_and_vali_df.fold.isna().any())   \n",
    "\n",
    "            for fold_idx in range(0, n_folds):\n",
    "                train_df = train_and_vali_df[train_and_vali_df.fold != fold_idx]\n",
    "                val_df = train_and_vali_df[train_and_vali_df.fold == fold_idx]\n",
    "                all_dfs:list[pd.DataFrame] = [train_df, val_df, test_df]\n",
    "                all_df = pd.concat(all_dfs)\n",
    "                check_unique(all_df)\n",
    "\n",
    "                train_df.to_csv(basename + f'_train_{test_idx}_{ratio:.2f}_{fold_idx}.csv')\n",
    "                val_df.to_csv(basename + f'_val_{test_idx}_{ratio:.2f}_{fold_idx}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
