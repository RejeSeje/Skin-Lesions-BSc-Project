{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression (LR)\n",
    "\n",
    "This is the notebook we run to get the results for our LR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, ConfusionMatrixDisplay, confusion_matrix\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_file_pairs(base_path=\"data/lr/lr_splitted_aug_data\",\n",
    "                        test_sets=5, variations=[\"0.00\", \"0.25\", \"0.50\", \"0.75\", \"1.00\"], train_val_pairs=5):\n",
    "    # Here we create a dictionary that contains all 3 files. We call them triples\n",
    "    all_triples = {}\n",
    "\n",
    "    # We name all of the files for the LR using this pattern\n",
    "    for test_set in range(test_sets):\n",
    "        triples = []\n",
    "\n",
    "        # Generating test set filename\n",
    "        test_filename = f\"{base_path}/m_f_ca_nc_test_{test_set}.csv\"\n",
    "\n",
    "        for variation in variations:\n",
    "            for pair in range(train_val_pairs):\n",
    "                train_filename = f\"{base_path}/m_f_ca_nc_train_{test_set}_{variation}_{pair}.csv\"\n",
    "                val_filename = f\"{base_path}/m_f_ca_nc_val_{test_set}_{variation}_{pair}.csv\"\n",
    "                triples.append((train_filename, val_filename, test_filename))\n",
    "        \n",
    "        all_triples[f\"test_set_{test_set}\"] = triples\n",
    "    \n",
    "    return all_triples\n",
    "\n",
    "\n",
    "def load_dataset_tv(filepath):\n",
    "    # Here we load the dataset for training and validation \n",
    "    df = pd.read_csv(filepath)\n",
    "    X = df[[\"F2\", \"F11\", \"sat_var\", \"blue_veil_pixels\", \"avg_green_channel\", \n",
    "            \"mean_asymmetry\", \"F1\", \"average_hue\", \"compactness_x\", \"dom_hue\"]]\n",
    "    y = df[\"is_cancerous\"] \n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def load_dataset_test(filepath):\n",
    "    # Loading the dataset for test. We test for females and males separately\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    X = df[[\"F2\", \"F11\", \"sat_var\", \"blue_veil_pixels\", \"avg_green_channel\", \n",
    "            \"mean_asymmetry\", \"F1\", \"average_hue\", \"compactness_x\", \"dom_hue\", \"gender\"]]\n",
    "    y = df[[\"is_cancerous\", \"gender\"]]\n",
    "\n",
    "    X_male = X[X[\"gender\"] == \"MALE\"]\n",
    "    X_female = X[X[\"gender\"] == \"FEMALE\"]\n",
    "    \n",
    "    y_male = y[y[\"gender\"] == \"MALE\"]\n",
    "    y_female = y[y[\"gender\"] == \"FEMALE\"]\n",
    "\n",
    "    # Make the final df with our features and gold labels\n",
    "    y_male = y_male[\"is_cancerous\"]\n",
    "    y_female = y_female[\"is_cancerous\"]\n",
    "    y = df[\"is_cancerous\"]\n",
    "\n",
    "    X_male = X_male[[\"F2\", \"F11\", \"sat_var\", \"blue_veil_pixels\", \"avg_green_channel\", \n",
    "                     \"mean_asymmetry\", \"F1\", \"average_hue\", \"compactness_x\", \"dom_hue\"]]\n",
    "    X_female = X_female[[\"F2\", \"F11\", \"sat_var\", \"blue_veil_pixels\", \"avg_green_channel\", \n",
    "                         \"mean_asymmetry\", \"F1\", \"average_hue\", \"compactness_x\", \"dom_hue\"]]\n",
    "    X = df[[\"F2\", \"F11\", \"sat_var\", \"blue_veil_pixels\", \"avg_green_channel\", \n",
    "            \"mean_asymmetry\", \"F1\", \"average_hue\", \"compactness_x\", \"dom_hue\"]]\n",
    "\n",
    "    return X, y, X_female, X_male, y_female, y_male"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define where we can find the logged models, parameters, metrics etc., and under a specific name\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5049\")\n",
    "mlflow.set_experiment(\"Logistic_Regression_Augmented\")\n",
    "\n",
    "# For code reproducibility, we set a seed\"\n",
    "np.random.seed(4)\n",
    "\n",
    "def train_and_evaluate_model(file_triples):\n",
    "    warnings.filterwarnings('ignore', message=\"Setting penalty=None will ignore the C and l1_ratio parameters\", category=UserWarning)\n",
    "    warnings.filterwarnings('ignore', message=\"X has feature names, but StandardScaler was fitted without feature names\", category=UserWarning)\n",
    "    # Here we train and evaluate the model\n",
    "    results = []\n",
    "    count = 0 \n",
    "    for test_set, triples in file_triples.items():\n",
    "        for train_file, val_file, test_file in triples:\n",
    "            count += 1\n",
    "                    \n",
    "            X_train, y_train = load_dataset_tv(train_file)\n",
    "            X_val, y_val = load_dataset_tv(val_file)\n",
    "            X_test, y_test, X_test_female, X_test_male, y_test_female, y_test_male = load_dataset_test(test_file)\n",
    "\n",
    "            # This is where we start the MLFlow run\n",
    "            with mlflow.start_run(run_name=f\"logistic_regression_{count}\"):\n",
    "                # We define the model, standardise the data and create a pipeline for it\n",
    "                logistic = LogisticRegression()\n",
    "                scaler = StandardScaler()\n",
    "                pipe = Pipeline(steps=[(\"scaler\", scaler), (\"logistic\", logistic)])\n",
    "\n",
    "                # We combine the training and validation data to create the predefined split\n",
    "                # using our own folds for the cross-validation in GridSearchCV\n",
    "                combined_X = np.vstack((X_train, X_val))\n",
    "                combined_y = pd.concat([y_train, y_val])\n",
    "                split_index = [-1] * len(X_train) + [0] * len(X_val)\n",
    "                pds = PredefinedSplit(test_fold=split_index)\n",
    "\n",
    "                # We define the parameters for the GridSearchCV\n",
    "                # We create two dictionaries for the parameter combiantions, as solver \"lbfgs\"\n",
    "                # can ONLY use l2 penalty, and solver \"liblinear\" can use both l1 and l2\n",
    "                parameters = [\n",
    "                    {\"logistic__solver\": [\"liblinear\"],\n",
    "                    \"logistic__penalty\": [\"l1\", \"l2\"],\n",
    "                    \"logistic__fit_intercept\": [True, False],\n",
    "                    \"logistic__C\": [0.01, 0.05, 0.1, 0.5, 1, 2, 5],\n",
    "                    \"logistic__class_weight\": [\"balanced\", None],\n",
    "                    \"logistic__max_iter\": [50, 100, 500, 1000]},\n",
    "                    {\"logistic__solver\": [\"lbfgs\"],\n",
    "                    \"logistic__penalty\": [\"l2\", None],\n",
    "                    \"logistic__fit_intercept\": [True, False],\n",
    "                    \"logistic__C\": [0.01, 0.05, 0.1, 0.5, 1, 2, 5],\n",
    "                    \"logistic__class_weight\": [\"balanced\", None],\n",
    "                    \"logistic__max_iter\": [50, 100, 500, 1000]}\n",
    "                    ]\n",
    "\n",
    "                # We perform the GridSearchCV and define it as \"search)\"\n",
    "                grid_search = GridSearchCV(estimator=pipe,\n",
    "                                           param_grid=parameters,\n",
    "                                           cv=pds,\n",
    "                                           scoring=\"accuracy\")\n",
    "                search = grid_search.fit(combined_X, combined_y)\n",
    "\n",
    "                # We predict the whole test data, and for male and female separately\n",
    "                # This is to find the accuracy, and other metrics\n",
    "                y_pred = search.predict(X_test)\n",
    "                y_pred_male = search.predict(X_test_male)\n",
    "                y_pred_female = search.predict(X_test_female)\n",
    "\n",
    "                # We predict the probabilities to get the AUROC score\n",
    "                y_score = search.predict_proba(X_test)[:, 1]\n",
    "                y_score_female = search.predict_proba(X_test_female)[:, 1]\n",
    "                y_score_male = search.predict_proba(X_test_male)[:, 1]\n",
    "                auroc = roc_auc_score(y_test, y_score)\n",
    "                auroc_female = roc_auc_score(y_test_female, y_score_female)\n",
    "                auroc_male = roc_auc_score(y_test_male, y_score_male)\n",
    "\n",
    "                # We calculate the accuracy for all groups\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                accuracy_female = accuracy_score(y_test_female, y_pred_female)\n",
    "                accuracy_male = accuracy_score(y_test_male, y_pred_male)\n",
    "\n",
    "                # We print the results\n",
    "                print(\"Accuracy for all groups:\", accuracy)\n",
    "                print(\"Best gridsearch score\", search.best_score_)\n",
    "                print(f\"Penalty {search.best_params_[\"logistic__penalty\"]}, \\\n",
    "                Solver {search.best_params_[\"logistic__solver\"]}, \\\n",
    "                Fit intercept {search.best_params_[\"logistic__fit_intercept\"]}, \\\n",
    "                C {search.best_params_[\"logistic__C\"]}, \\\n",
    "                Class weight {search.best_params_[\"logistic__class_weight\"]}, \\\n",
    "                Max iterations {search.best_params_[\"logistic__max_iter\"]}\")\n",
    "                \n",
    "                # Code to run confusion matrices for the whole test data \n",
    "                # cm = confusion_matrix(y_test, y_pred)\n",
    "                # cm = ConfusionMatrixDisplay(cm)\n",
    "                # cm = cm.plot()\n",
    "                # plt.savefig(f\"../../analysis/plots/cm_plots/confusion_matrix_{count}.png\")\n",
    "                # plt.show()\n",
    "\n",
    "                # We log the confusion matrices to MLFlow\n",
    "                # mlflow.log_artifacts(f\"confusion_matrix_{count}.png\", \"confusion_matrices\")\n",
    "\n",
    "\n",
    "                # We log the metrics and parameters to MLFlow\n",
    "                metrics = [\n",
    "                (\"AUROC\", auroc),\n",
    "                (\"AUROC_female\", auroc_female),\n",
    "                (\"AUROC_male\", auroc_male),\n",
    "                (\"Accuracy\", accuracy),\n",
    "                (\"Accuracy_female\", accuracy_female),\n",
    "                (\"Accuracy_male\", accuracy_male),\n",
    "                (\"Recall\", recall_score(y_test, y_pred)),\n",
    "                (\"Precision\", precision_score(y_test, y_pred)),\n",
    "                (\"F1-score\", f1_score(y_test, y_pred))\n",
    "                ]\n",
    "\n",
    "                for name, value in metrics:\n",
    "                    mlflow.log_metric(name, value)\n",
    "                \n",
    "                # We log parameters\n",
    "                mlflow.log_params(search.best_params_)\n",
    "\n",
    "                # We define what we want in the output file\n",
    "                # Variation shows the sex ratio of the training data\n",
    "                variation = train_file.split(\"_\")[-2] \n",
    "                results.append({\n",
    "                    \"variation\": variation,\n",
    "                    \"accuracy\": accuracy,\n",
    "                    \"accuracy_female\": accuracy_female,\n",
    "                    \"accuracy_male\": accuracy_male,\n",
    "                    \"auroc\": roc_auc_score(y_test, y_score), \n",
    "                    \"auroc_female\": roc_auc_score(y_test_female, y_score_female),\n",
    "                    \"auroc_male\": roc_auc_score(y_test_male, y_score_male),\n",
    "                    \"Recall\": recall_score(y_test, y_pred),\n",
    "                    \"Precision\": precision_score(y_test, y_pred),\n",
    "                    \"F1-score\": f1_score(y_test, y_pred),\n",
    "                    \"best_params\": search.best_params_,\n",
    "                    \"best_score\": search.best_score_\n",
    "                })\n",
    "\n",
    "\n",
    "                # Here we log the model to MLFlow (if we want to)\n",
    "                # mlflow.sklearn.log_model(search, \"Logistic_regression_model\")\n",
    "\n",
    "    # We convert the results to a dataframe\n",
    "    results_df = pd.DataFrame(results)\n",
    "    # We save the results to a csv file\n",
    "    results_df.to_csv(\"../../data/results/lr_results/lr_results_aug.csv\", index=False)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../../data/lr/lr_splitted_aug_data\"\n",
    "all_file_triples = generate_file_pairs(base_path)\n",
    "results = train_and_evaluate_model(all_file_triples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelorenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
